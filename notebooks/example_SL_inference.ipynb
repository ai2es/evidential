{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml, joblib\n",
    "from evml.model import DNN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_config = \"/glade/work/schreck/repos/evidential/evidential/results/mlp/model.yml\"\n",
    "with open(mlp_config) as cf:\n",
    "    mlp_conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = mlp_conf[\"save_loc\"]\n",
    "input_cols = mlp_conf[\"input_cols\"]\n",
    "output_cols = mlp_conf[\"output_cols\"]\n",
    "input_size = len(input_cols)\n",
    "middle_size = mlp_conf[\"middle_size\"]\n",
    "output_size = mlp_conf[\"output_size\"]\n",
    "batch_size = mlp_conf[\"batch_size\"]\n",
    "dropout = mlp_conf[\"dropout\"]\n",
    "batch_norm = mlp_conf[\"batch_norm\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(mlp_conf[\"idaho\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"day\"] = data[\"Time\"].apply(lambda x: str(x).split(\" \")[0])\n",
    "flat_seed = 1000\n",
    "\n",
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state = flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(data, groups = data[\"day\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = data.iloc[train_index].copy(), data.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=100,  random_state = flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups = train_data[\"day\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index] .copy()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained x-scaler and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{save_loc}/scalers.pkl\", \"rb\") as fid:\n",
    "    scaler_x, scaler_y = joblib.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler_x.transform(train_data[input_cols])\n",
    "x_valid = scaler_x.transform(valid_data[input_cols])\n",
    "x_test  = scaler_x.transform(test_data[input_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DNN(\n",
    "    input_size, \n",
    "    output_size, \n",
    "    block_sizes = [middle_size], \n",
    "    dr = [dropout], \n",
    "    batch_norm = batch_norm, \n",
    "    lng = False\n",
    ").to(device)\n",
    "\n",
    "_ model.load_weights(f\"{save_loc}/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_splits = [x_train, x_valid, x_test]\n",
    "splits = [train_data, valid_data, test_data]\n",
    "        \n",
    "for x,y in zip(x_splits, splits):\n",
    "    y_pred = model.predict(x, batch_size = batch_size)\n",
    "    y[\"y_pred\"] = scaler_y.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next up the evidential regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loc = \"/glade/work/schreck/repos/evidential/evidential/results/evmlp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{save_loc}/scalers.pkl\", \"rb\") as fid:\n",
    "    scaler_x, scaler_y = joblib.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler_x.transform(train_data[input_cols])\n",
    "x_valid = scaler_x.transform(valid_data[input_cols])\n",
    "x_test  = scaler_x.transform(test_data[input_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(\n",
    "    input_size, \n",
    "    output_size, \n",
    "    block_sizes = [middle_size], \n",
    "    dr = [dropout], \n",
    "    batch_norm = batch_norm, \n",
    "    lng = True\n",
    ").to(device)\n",
    "\n",
    "_ = model.load_weights(f\"{save_loc}/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute variance in the training outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = train_data[output_cols[0]].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_splits = [x_train, x_valid, x_test]\n",
    "splits = [train_data, valid_data, test_data]\n",
    "        \n",
    "for x,y in zip(x_splits, splits):\n",
    "    y_pred = model.predict(x, batch_size = batch_size)              \n",
    "    y[\"mu\"] = scaler_y.inverse_transform(y_pred[:,0:1])\n",
    "    y[\"v\"] = y_pred[:,1:2]\n",
    "    y[\"alpha\"] = y_pred[:,2:3]\n",
    "    y[\"beta\"] = y_pred[:,3:4]\n",
    "\n",
    "    inverse_evidence = 1. / ((y[\"alpha\"].copy()-1) * y[\"v\"])\n",
    "    variance = y[\"beta\"] * inverse_evidence\n",
    "\n",
    "    y[\"var\"] = (variance * sigma**2)\n",
    "    y[\"conf\"] = (inverse_evidence * sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
