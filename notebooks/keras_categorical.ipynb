{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e2822e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36cfe4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = \"../config/p-type.yml\"\n",
    "with open(config) as cf:\n",
    "    conf = yaml.load(cf, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "630b9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation=tf.nn.relu)\n",
    "        self.dense3 = tf.keras.layers.Dense(4, activation=\"linear\")\n",
    "        self.dropout = tf.keras.layers.Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.dense1(inputs)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x, training=training)\n",
    "        x = self.dense3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e184dd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(conf['data_path'], \"cached.parquet\")):\n",
    "    df = pd.concat([\n",
    "        pd.read_parquet(x) for x in tqdm.tqdm(glob.glob(os.path.join(conf['data_path'], \"*.parquet\")))\n",
    "    ])\n",
    "    df.to_parquet(os.path.join(conf['data_path'], \"cached.parquet\"))\n",
    "else:\n",
    "    df = pd.read_parquet(os.path.join(conf['data_path'], \"cached.parquet\"))\n",
    "\n",
    "### Split and preprocess the data\n",
    "df['day'] = df['datetime'].apply(lambda x: str(x).split(' ')[0])\n",
    "df[\"id\"] = range(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6f9905df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need the same test_data for all trained models (data and model ensembles)\n",
    "n_splits = 1\n",
    "flat_seed = 1000\n",
    "data_seed = 0\n",
    "gsp = GroupShuffleSplit(n_splits=1,  random_state = flat_seed, train_size=0.9)\n",
    "splits = list(gsp.split(df, groups = df[\"day\"]))\n",
    "train_index, test_index = splits[0]\n",
    "train_data, test_data = df.iloc[train_index].copy(), df.iloc[test_index].copy() \n",
    "\n",
    "# Make N train-valid splits using day as grouping variable\n",
    "gsp = GroupShuffleSplit(n_splits=n_splits,  random_state = flat_seed, train_size=0.885)\n",
    "splits = list(gsp.split(train_data, groups = train_data[\"day\"]))\n",
    "train_index, valid_index = splits[data_seed]\n",
    "train_data, valid_data = train_data.iloc[train_index].copy(), train_data.iloc[valid_index] .copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "369b117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = conf['tempvars'] + conf['tempdewvars'] + conf['ugrdvars'] + conf['vgrdvars']\n",
    "outputs = conf['outputvars']\n",
    "num_classes = len(outputs)\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "x_train = scaler_x.fit_transform(train_data[features])\n",
    "x_valid = scaler_x.transform(valid_data[features])\n",
    "x_test = scaler_x.transform(test_data[features])\n",
    "y_train = tf.keras.utils.to_categorical(np.argmax(train_data[outputs].to_numpy(), 1), num_classes)\n",
    "y_valid = tf.keras.utils.to_categorical(np.argmax(valid_data[outputs].to_numpy(), 1), num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(np.argmax(test_data[outputs].to_numpy(), 1), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d8c0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgamma = tf.math.lgamma\n",
    "digamma = tf.math.digamma\n",
    "\n",
    "epochs = [1]\n",
    "\n",
    "def KL(alpha, num_classes=4):\n",
    "    one = K.constant(np.ones((1,num_classes)),dtype=tf.float32)\n",
    "    S = K.sum(alpha,axis=1,keepdims=True)  \n",
    "    A1 = K.sum(lgamma(alpha), axis=1, keepdims=True)\n",
    "    A2 = K.sum(lgamma(one), axis=1, keepdims=True)\n",
    "    A3 = lgamma(K.sum(one, axis=1, keepdims=True))\n",
    "    A4 = K.sum((alpha - one)*(digamma(alpha)-digamma(S)), axis=1, keepdims=True)\n",
    "    kl = lgamma(S) - A1 + A2 - A3 + A4\n",
    "          \n",
    "    return kl\n",
    "\n",
    "def loss_func(y_true, output):\n",
    "    y_evidence = K.relu(output)\n",
    "    alpha = y_evidence+1\n",
    "    S = K.sum(alpha,axis=1,keepdims=True)\n",
    "    p = alpha / S  \n",
    "\n",
    "    err = K.sum(K.pow((y_true-p),2),axis=1,keepdims=True)\n",
    "    var = K.sum(alpha*(S-alpha)/(S*S*(S+1)),axis=1,keepdims=True)\n",
    "    \n",
    "    l = K.sum(err + var, axis=1, keepdims=True)\n",
    "    l = K.sum(l)\n",
    "    \n",
    "    kl =  K.minimum(1.0, epochs[0]/50) * K.sum(KL((1-y_true)*(alpha)+y_true))\n",
    "    return l + kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f6e1b38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23200"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del mlp\n",
    "K.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2bc5226b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(units = 128, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units = 128, activation = 'relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(units = 4, activation = 'linear'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bc33c4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "8c70db4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "mlp.compile(\n",
    "    optimizer = optimizer, \n",
    "    loss = loss_func,\n",
    "    metrics = ['accuracy'],\n",
    "    #run_eagerly=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "1daadf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create early stopping callback\n",
    "callback1 = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_accuracy', \n",
    "    mode = 'max',\n",
    "    patience = 15,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "# create reduce LR callback\n",
    "callback2 = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor = 'val_accuracy', \n",
    "    factor = 0.1, \n",
    "    patience = 3, \n",
    "    verbose = 0,\n",
    "    mode = 'max',\n",
    "    min_lr = 1e-7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "4a8bea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "176/176 - 1s - loss: 549.6086 - accuracy: 0.8416 - val_loss: 489.8992 - val_accuracy: 0.8459 - lr: 0.0010 - 1s/epoch - 8ms/step\n",
      "Epoch 2/1000\n",
      "176/176 - 1s - loss: 406.2060 - accuracy: 0.8804 - val_loss: 459.3554 - val_accuracy: 0.8586 - lr: 0.0010 - 522ms/epoch - 3ms/step\n",
      "Epoch 3/1000\n",
      "176/176 - 1s - loss: 387.1797 - accuracy: 0.8862 - val_loss: 453.8924 - val_accuracy: 0.8613 - lr: 0.0010 - 506ms/epoch - 3ms/step\n",
      "Epoch 4/1000\n",
      "176/176 - 1s - loss: 377.1010 - accuracy: 0.8895 - val_loss: 444.6345 - val_accuracy: 0.8642 - lr: 0.0010 - 503ms/epoch - 3ms/step\n",
      "Epoch 5/1000\n",
      "176/176 - 1s - loss: 370.1025 - accuracy: 0.8918 - val_loss: 444.8278 - val_accuracy: 0.8628 - lr: 0.0010 - 506ms/epoch - 3ms/step\n",
      "Epoch 6/1000\n",
      "176/176 - 1s - loss: 366.3386 - accuracy: 0.8928 - val_loss: 431.2420 - val_accuracy: 0.8685 - lr: 0.0010 - 508ms/epoch - 3ms/step\n",
      "Epoch 7/1000\n",
      "176/176 - 1s - loss: 362.2213 - accuracy: 0.8940 - val_loss: 434.3902 - val_accuracy: 0.8685 - lr: 0.0010 - 506ms/epoch - 3ms/step\n",
      "Epoch 8/1000\n",
      "176/176 - 0s - loss: 359.8517 - accuracy: 0.8948 - val_loss: 435.0177 - val_accuracy: 0.8668 - lr: 0.0010 - 498ms/epoch - 3ms/step\n",
      "Epoch 9/1000\n",
      "176/176 - 0s - loss: 357.0865 - accuracy: 0.8956 - val_loss: 430.7372 - val_accuracy: 0.8684 - lr: 0.0010 - 499ms/epoch - 3ms/step\n",
      "Epoch 10/1000\n",
      "176/176 - 1s - loss: 351.5455 - accuracy: 0.8973 - val_loss: 429.3126 - val_accuracy: 0.8696 - lr: 1.0000e-04 - 507ms/epoch - 3ms/step\n",
      "Epoch 11/1000\n",
      "176/176 - 1s - loss: 349.5153 - accuracy: 0.8980 - val_loss: 429.2841 - val_accuracy: 0.8694 - lr: 1.0000e-04 - 505ms/epoch - 3ms/step\n",
      "Epoch 12/1000\n",
      "176/176 - 0s - loss: 348.0580 - accuracy: 0.8987 - val_loss: 425.5870 - val_accuracy: 0.8690 - lr: 1.0000e-04 - 500ms/epoch - 3ms/step\n",
      "Epoch 13/1000\n",
      "176/176 - 1s - loss: 347.0499 - accuracy: 0.8986 - val_loss: 423.1553 - val_accuracy: 0.8695 - lr: 1.0000e-04 - 502ms/epoch - 3ms/step\n",
      "Epoch 14/1000\n",
      "176/176 - 1s - loss: 345.9630 - accuracy: 0.8991 - val_loss: 423.1402 - val_accuracy: 0.8695 - lr: 1.0000e-05 - 503ms/epoch - 3ms/step\n",
      "Epoch 15/1000\n",
      "176/176 - 1s - loss: 345.9655 - accuracy: 0.8989 - val_loss: 422.8365 - val_accuracy: 0.8698 - lr: 1.0000e-05 - 504ms/epoch - 3ms/step\n",
      "Epoch 16/1000\n",
      "176/176 - 1s - loss: 345.4326 - accuracy: 0.8989 - val_loss: 422.6827 - val_accuracy: 0.8696 - lr: 1.0000e-05 - 508ms/epoch - 3ms/step\n",
      "Epoch 17/1000\n",
      "176/176 - 1s - loss: 345.6244 - accuracy: 0.8990 - val_loss: 422.6067 - val_accuracy: 0.8698 - lr: 1.0000e-05 - 502ms/epoch - 3ms/step\n",
      "Epoch 18/1000\n",
      "176/176 - 1s - loss: 345.4770 - accuracy: 0.8992 - val_loss: 422.3115 - val_accuracy: 0.8699 - lr: 1.0000e-05 - 502ms/epoch - 3ms/step\n",
      "Epoch 19/1000\n",
      "176/176 - 1s - loss: 345.3849 - accuracy: 0.8993 - val_loss: 422.5494 - val_accuracy: 0.8699 - lr: 1.0000e-05 - 507ms/epoch - 3ms/step\n",
      "Epoch 20/1000\n",
      "176/176 - 1s - loss: 345.7690 - accuracy: 0.8989 - val_loss: 422.5851 - val_accuracy: 0.8698 - lr: 1.0000e-05 - 506ms/epoch - 3ms/step\n",
      "Epoch 21/1000\n",
      "176/176 - 1s - loss: 345.1887 - accuracy: 0.8992 - val_loss: 422.5110 - val_accuracy: 0.8698 - lr: 1.0000e-05 - 500ms/epoch - 3ms/step\n",
      "Epoch 22/1000\n",
      "176/176 - 1s - loss: 344.7422 - accuracy: 0.8993 - val_loss: 422.4634 - val_accuracy: 0.8698 - lr: 1.0000e-06 - 500ms/epoch - 3ms/step\n",
      "Epoch 23/1000\n",
      "176/176 - 1s - loss: 345.1736 - accuracy: 0.8993 - val_loss: 422.4188 - val_accuracy: 0.8697 - lr: 1.0000e-06 - 508ms/epoch - 3ms/step\n",
      "Epoch 24/1000\n",
      "176/176 - 1s - loss: 345.1698 - accuracy: 0.8994 - val_loss: 422.3876 - val_accuracy: 0.8697 - lr: 1.0000e-06 - 507ms/epoch - 3ms/step\n",
      "Epoch 25/1000\n",
      "176/176 - 1s - loss: 344.6694 - accuracy: 0.8995 - val_loss: 422.3893 - val_accuracy: 0.8697 - lr: 1.0000e-07 - 508ms/epoch - 3ms/step\n",
      "Epoch 26/1000\n",
      "176/176 - 0s - loss: 344.8584 - accuracy: 0.8992 - val_loss: 422.3813 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 496ms/epoch - 3ms/step\n",
      "Epoch 27/1000\n",
      "176/176 - 1s - loss: 345.1970 - accuracy: 0.8994 - val_loss: 422.3813 - val_accuracy: 0.8697 - lr: 1.0000e-07 - 501ms/epoch - 3ms/step\n",
      "Epoch 28/1000\n",
      "176/176 - 1s - loss: 345.2709 - accuracy: 0.8992 - val_loss: 422.3811 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 505ms/epoch - 3ms/step\n",
      "Epoch 29/1000\n",
      "176/176 - 1s - loss: 345.3783 - accuracy: 0.8989 - val_loss: 422.3786 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 508ms/epoch - 3ms/step\n",
      "Epoch 30/1000\n",
      "176/176 - 1s - loss: 344.9865 - accuracy: 0.8992 - val_loss: 422.3745 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 506ms/epoch - 3ms/step\n",
      "Epoch 31/1000\n",
      "176/176 - 1s - loss: 345.0533 - accuracy: 0.8995 - val_loss: 422.3753 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 501ms/epoch - 3ms/step\n",
      "Epoch 32/1000\n",
      "176/176 - 1s - loss: 345.0529 - accuracy: 0.8994 - val_loss: 422.3711 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 512ms/epoch - 3ms/step\n",
      "Epoch 33/1000\n",
      "176/176 - 1s - loss: 345.1215 - accuracy: 0.8993 - val_loss: 422.3663 - val_accuracy: 0.8698 - lr: 1.0000e-07 - 516ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "# fit model to training data\n",
    "history = mlp.fit(\n",
    "    x = x_train, \n",
    "    y = y_train,\n",
    "    validation_data = (x_valid, y_valid),\n",
    "    batch_size = conf[\"trainer\"][\"batch_size\"],\n",
    "    epochs = 1000,\n",
    "    callbacks = [callback1, callback2],\n",
    "    verbose = 2,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d1632186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prob_uncertinty(outputs):\n",
    "    evidence = K.relu(outputs)\n",
    "    alpha = evidence + 1\n",
    "    u = 4 / K.sum(alpha, axis=1, keepdims=True)\n",
    "    prob = alpha / K.sum(alpha, axis=1, keepdims=True)\n",
    "    return prob.numpy(), u.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "30945c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1261/1261 [==============================] - 1s 865us/step\n"
     ]
    }
   ],
   "source": [
    "preds = mlp.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d7515525",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob, u = calc_prob_uncertinty(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "3376aaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.97840726, 0.00719758, 0.00719758, 0.00719758],\n",
       "        [0.9840467 , 0.00531777, 0.00531777, 0.00531777],\n",
       "        [0.97950155, 0.00683282, 0.00683282, 0.00683282],\n",
       "        ...,\n",
       "        [0.953632  , 0.015456  , 0.015456  , 0.015456  ],\n",
       "        [0.9398522 , 0.02004927, 0.02004927, 0.02004927],\n",
       "        [0.9856658 , 0.00477808, 0.00477808, 0.00477808]], dtype=float32),\n",
       " array([[0.0287903 ],\n",
       "        [0.02127108],\n",
       "        [0.02733126],\n",
       "        ...,\n",
       "        [0.06182401],\n",
       "        [0.08019707],\n",
       "        [0.0191123 ]], dtype=float32))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob, u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755009e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evidential",
   "language": "python",
   "name": "evidential"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
